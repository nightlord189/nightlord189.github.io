+++ 
draft = false
date = 2021-03-04T13:55:09+06:00
title = "Установка ELK"
slug = "" 
tags = []
categories = []
thumbnail = "images/tn.png"
description = ""
+++

Поднял я тут на днях для своих проектов ElasticSearch + Kibana на VPS, так что запишу сюда, что делал и что получилось.

Цель изначально была - начать переписывать один мой проект, который давно поддерживаю. Но с чего начать чистый лист нового бэкенда? Конечно с логов! А куда сейчас логируют модные посоны? В Elastic.

Сперва я поднял все это дело локально на Windows 10 в Docker. И оно поднялось и заработало! Правда, выжрало немало так RAM и я впервые увидел потребление 15/16 гб на ноуте.

Далее арендовал VPS в Hetzner. Убедился, что 2 ГБ RAM явно недостаточно для всего ELK - докер вроде бы поднимался, но сразу же тух и вменяемого ответа в браузере получить не удалось. Увеличил тариф до 4 Гб RAM, дело пошло. 
Путем гуглежа, просмотра stackoverflow и гайдов наваял вот такой docker-compose.yml:

docker-compose.yml
```
version: '3.7'

services:
  elasticsearch:
    image: elasticsearch:7.9.3
    ports:
      - '9200:9200'
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - ELASTIC_PASSWORD=<password>
      - xpack.security.enabled=true
    ulimits:
      memlock:
        soft: -1
        hard: -1
    restart: always
    volumes:
      - esdata1:/usr/share/elasticsearch/data
  kibana:
    image: kibana:7.9.3
    ports:
      - '5601:5601'
    volumes:
      - ./kibana.yml:/usr/share/kibana/config/kibana.yml
    restart: always

volumes:
  esdata1:
    driver: local
```

Хотел бы обойтись одним конфигом, но для авторизации пришлось так же разместить и конфиг кибаны (не захотела она принимать пароль через environment в docker-compose. Вроде как раньше год назад это работало, теперь уже нет).

kibana.yml
```
server.host: "0.0.0.0"
elasticsearch.hosts: ["http://elasticsearch:9200"]
elasticsearch.username: "elastic"
elasticsearch.password: "<password>"
```

Итого чтобы поднять все это дело на VPS/локально:
1. Размещаем файлы docker-compose и kibana.yml в директории
2. Вызываем sudo docker-compose up -d
3. Через некоторое время проверяем, поднялся ли Elastic по адресу:
http://host:9200
и Kibana:
http://host:5601
4. Сохраняем тестовый документ (не забываем выставить Basic Auth, если включили авторизацию):  
POST http://host:9200/company/employee/1/_create 
```
{
    "name": "John1",
    "age": 25,
    "experienceInYears": 4,
    "component": "test.0111"
}
```
5. В Kibana настраиваем первый индекс: меню слева -> Kibana -> Discover
6. Можем уже искать среди нашей одной записи :)

**Пояснения**
+ Поднимаю только сам Elastic и Kibana. Logstash не нужен, логировать буду через logrus сразу в Elastic.
+ "ES_JAVA_OPTS=-Xms512m -Xmx512m" - судя по stackoverflow, вроде как с этим Elastic должен поумерить свои аппетиты к RAM (а вообще это настройки Java-машины в целом).
+ Вообще не понял, как работают ulimits, краткого и внятного пояснения я не смог найти. Возможно, тупой или плохо искал. Ну, главное что работает.
+ Добавил restart:always, чтобы при перезапуске VPS все поднималось само
+ Добавил volume, чтобы при перезапуске данные Elastic-а не удалялись
+ Добавил авторизацию, битый час с ней морочился, но в итоге заработало. А то все же как-то опасно выставлять сервис наружу в интернет вообще без авторизации.

**Что не получилось сделать/нужно сделать в будущем**
+ Ограничение размера volume
+ Автоматическое удаление старых записей (т.к. мне ELK нужен только для логирования - логи не такая ценная информация и старше 7-14 дней мне нафиг не нужны). Судя по ответам в гугле - многие рекомендуют через cron делать вызовы API самого Elastic-а по удалению записей. Но при этом в записи должно быть какое-то полей с датой, чтобы можно было по нему фильтровать. В общем, жаль, что нет какого-то встроенного и простого функционала по автоудалению всех записей старше n дней.

P.S. Параллельно поднял Elastic также на AWS в рамках Free Tier, но пользоваться, наверное, все же буду тем, что на VPS. В следующих заметках попробую объяснить свои впечатления от знакомства с AWS.